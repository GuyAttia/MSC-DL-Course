r"""
Use this module to write your answers to the questions in the notebook.

Note: Inside the answer strings you can use Markdown format and also LaTeX
math (delimited with $$).
"""

# ==============
# Part 1 answers

part1_q1 = r"""
**Your answer:**
"""

part1_q2 = r"""
**Your answer:**
"""

# ==============
# Part 2 answers

part2_q1 = r"""
**Your answer:**
"""

part2_q2 = r"""
**Your answer:**
"""

# ==============

# ==============
# Part 3 answers

part3_q1 = r"""
**Your answer:**
"""

part3_q2 = r"""
**Your answer:**
"""

part3_q3 = r"""
**Your answer:**
"""

# ==============

# ==============
# Part 4 answers

part4_q1 = r"""
We can see in the final plot that the residuals are closer to 0 comparing to the top 5 features used before, means that the prediction error is closer to zero. We can conclude that the cross-validation method shown in the final plot produces better results than linear regression over the top-5 features.
"""

part4_q2 = r"""
1. Yes, the non linear features were added by us, and they are still used as the regular features with the weights matrix.
2. Yes, By moving to higher dimension (need to be careful not too high for complexity manners), we can engineer any non linear relation between features and use it in linear regression.
3. It still will be an hyperplane, the engineering of the new features will cause the parameters to change and therefore to change the decision boundary.
"""

part4_q3 = r"""
1. Using logarithmic scale instead of linear scale helps us see more information under smaller area. One can think of it as it 'shrinks' the linear space, so more data can be reviewed.
2. number of folds = 3; degree range = 1,2,3; lambda_range=20;
In conclusion- the model was fitted 180 times.
"""

# ==============
